{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 - Detect Network Attacks Using Clustering Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Clustering?**\n",
    "\n",
    "\n",
    "***Clustering*** is an unsupervised machine learning technique that divides a population or data points into several groups. Data points in the same groups are more like other data points in the same group and dissimilar to the data points in different groups. \n",
    "\n",
    "\n",
    "![Clustering Example](imgs/clustering_example.jpg)\n",
    "\n",
    "\n",
    "**Why Clustering?**\n",
    "Clustering is crucial as it determines the intrinsic grouping among the unlabeled data present. There are no criteria for good clustering. It depends on the user, what is criteria they may use which satisfy their need. \n",
    "\n",
    "For instance, we could be interested in finding representatives for homogeneous groups (data reduction). Another example is when you want to look for “natural clusters” and describe their unknown properties (“natural” data types). Also, you could find valuable and suitable groupings (“useful” data classes) or find unusual data objects (outlier detection). Consequently, the clustering algorithm must make assumptions that constitute the similarity of points, and each assumption makes different and equally valid clusters.\n",
    "\n",
    "**Applications of Clustering in different fields**\n",
    "-\tMarket segmentation;\n",
    "-\tSocial network analysis;\n",
    "-\tSearch result grouping;\n",
    "-\tMedical imaging;\n",
    "-\tImage segmentation; and\n",
    "-\tAnomaly detection.\n",
    "\n",
    "**Clustering Analysis in Cyber Security**\n",
    "\n",
    "As we already cited, cybersecurity considers that the system is in an environment in the presence of an enemy, which continues plans to attack your data integrity and obfuscate your action in your system. Given this situation, it is a good idea to try to group similar data in a cluster, making it possible to, after this pre-processing, you start to try to set labels in the data.\n",
    "\n",
    "There are a set of papers that presents ideas on how to use clustering in cybersecurity problems. To increase your knowledge, you can reed these materials:\n",
    "\n",
    "- [Using Cluster Analysis for Comprehensive Threat Detection](http://secbi.com/white-paper-form/SecBI_White_Paper_Using_Cluster_Analysis_for_Comprehensive_Threat_Detection.pdf)\n",
    "- [System log clustering approaches for cyber security applications: A survey](https://www.sciencedirect.com/science/article/pii/S0167404820300250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - Clustering Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process to analyze a clustering following these steps:\n",
    "\n",
    "- Collecting Data;\n",
    "- Prepare data;\n",
    "- Create similarity metric;\n",
    "- Run clustering algorithm; and\n",
    "- Interpret results and adjust your clustering.\n",
    "\n",
    "![Clustering Analysis Process](imgs/clustering_analysis_process.PNG)\n",
    "\n",
    "**Collecting Data**\n",
    "As in any ML solution, the first part of the work is to define which type and amount of data are required to achieve good results. In cyber, because of the malicious user presence, this challenge becomes much more complicated.\n",
    "\n",
    "The first reason is that in a real scenario is required to install multiples sensors, which generate similar data most of the time; however, a specific slot of time can generate evidence of an attack that needs to be fused to identify the malicious event correctly.\n",
    "\n",
    "It is a hard task because it requires multiple skills; for example, system operational, network, machine language, reverse engineering, etc.\n",
    "\n",
    "To simplify the task of a Data Scientist to discover how to use different and innovative ML techniques inside the cyber domain, DARPA created a dataset named NSL-KDD. The first version, which was named KDD99, had a set of duplicate information, and its size is a little big, which makes it hard to apply it in ML systems.\n",
    "\n",
    "DARPA has collected over nine weeks and consists of raw tcpdump traffic in a local area network (LAN) that simulates a typical United States Air Force LAN environment. Some network attacks were deliberately carried out during the recording period. \n",
    "\n",
    "The general attacks categories were NLS-KDD has are:\n",
    "- dos: Denial of Service;\n",
    "- r2l: Unauthorized access from remote servers;\n",
    "- u2r: Privilege escalation attempts;\n",
    "- probe: Brute-force probing attacks; and\n",
    "- normal: normal traffic (no attacks).\n",
    "\n",
    "However, in the dataset, you did not find this classification; you find the name of the attack, as it is presented in the Figure below:  \n",
    "\n",
    "![NSL-KDD Attack Types](imgs/ndd_attack_types.jpg)\n",
    "\n",
    "\n",
    "**Prepare Data**\n",
    "\n",
    "As with any ML problem, you must normalize, scale, and transform feature data. While clustering however, you must additionally ensure that the prepared data lets you accurately calculate the similarity between examples.\n",
    "\n",
    "**Create Similarity Metric**\n",
    "\n",
    "Before a clustering algorithm can group data, it needs to know how similar pairs of examples are. You quantify the similarity between examples by creating a similarity metric. Creating a similarity metric requires you to carefully understand your data and how to derive similarity from your features.\n",
    "\n",
    "**Run Clustering Algorithm**\n",
    "\n",
    "A clustering algorithm uses the similarity metric to cluster data. This course focuses on k-means.\n",
    "\n",
    "**Interpret Results and Adjust**\n",
    "\n",
    "Checking the quality of your clustering output is iterative and exploratory because clustering lacks “truth” that can verify the output. You verify the result against expectations at the cluster-level and the example-level. Improving the result requires iteratively experimenting with the previous steps to see how they affect the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C - Clustering using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 - Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to load the dataset, adjusting it to be consume by the ML model. As we already used, the easiest way to load dataset is from pandas read_csv method. In this case, we will pass the columns names to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (125973, 43)\n",
      "test shape: (22544, 43)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(trainfile, testfile,\n",
    "                 header_names):\n",
    "    train_df = pd.read_csv(trainfile, names=header_names)\n",
    "    print('train shape:',train_df.shape)\n",
    "    \n",
    "    test_df = pd.read_csv(testfile, names=header_names)\n",
    "    print('test shape:',test_df.shape)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "#load dataset\n",
    "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "                    'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "                    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "                    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "                    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "                    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "                    'dst_host_srv_diff_host_rate',\n",
    "                    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "                    'dst_host_srv_rerror_rate',\n",
    "                    'attack_type', 'success_pred']\n",
    "\n",
    "train_file_name = 'resources/ndd/KDDTrain+.txt'\n",
    "test_file_name='resources/ndd/KDDTest+.txt'\n",
    "\n",
    "train_df, test_df= load_dataset(train_file_name, test_file_name,header_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously cited, the dataset does not contain the attack categories (*dos, r2l, u2r, probe, and normal*), but the attack's types. The number of attack types (40) is unfeasible to use the clustering approach because the model has its performance-sensitive as the number of required clusters.\n",
    "\n",
    "A new file is required to solve this situation, which contains the type and categories, like a dictionary.\n",
    "\n",
    "Its format is presented in the Figure. In this Figure, the first column is the attack type and the second is the category.\n",
    "\n",
    "![Training Attack Dictionary](imgs/data_dict.PNG)\n",
    "\n",
    "\n",
    "To create the dictionary, we define a method, which append the value 'benign' to normal data in the dataset and after import the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_dictionary(dictionary_name):\n",
    "    category = defaultdict(list)\n",
    "    category['benign'].append('normal')\n",
    "\n",
    "    with open(dictionary_name, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            attack, cat = line.strip().split(' ')\n",
    "            category[cat].append(attack)\n",
    "\n",
    "    mapping_dict = dict((v, k)\n",
    "                        for k in category for v in category[k])\n",
    "    return mapping_dict\n",
    "\n",
    "#load dictionary\n",
    "dict_file = 'resources/ndd/training_attack_types.txt'\n",
    "dicta = load_dictionary(dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to insert this new field in the imported daframe and drop the unnecessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d89fc990f86f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdrop_list_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdrop_list_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'success_pred'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0madjust_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'attack_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attack_category'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_list_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d89fc990f86f>\u001b[0m in \u001b[0;36madjust_datasets\u001b[1;34m(train_ds, test_ds, dicta, att_type_lb, catg_attk_lab, drop_list_label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madjust_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_type_lb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_list_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt_type_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt_type_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_list_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\opt\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \"\"\"\n\u001b[1;32m-> 3970\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3972\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\opt\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d89fc990f86f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madjust_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_type_lb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_list_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt_type_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcatg_attk_lab\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt_type_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_list_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def adjust_datasets(train_ds, test_ds, dicta, att_type_lb, catg_attk_lab,drop_list_label):\n",
    "    train_ds[catg_attk_lab] = train_ds[att_type_lb].map(lambda x: dicta[x])\n",
    "    test_ds[catg_attk_lab] = test_ds[att_type_lb].map(lambda x: dicta[x])\n",
    "\n",
    "    for i in range(0, len(drop_list_label)):\n",
    "        train_ds.drop([drop_list_label[i]], axis=1, inplace=True)\n",
    "        test_ds.drop([drop_list_label[i]], axis=1, inplace=True)\n",
    "\n",
    "drop_list_label=[]\n",
    "drop_list_label.append('success_pred')\n",
    "adjust_datasets(train_df,test_df,dicta,'attack_type', 'attack_category',drop_list_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we will define a set of index list that contains the type of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getColunssByClass(header_names):\n",
    "    nominal_idx = [1, 2, 3]\n",
    "    bin_idx = [6, 11, 13, 14, 20, 21]\n",
    "    numeric_idx = list(set(range(41)).difference(nominal_idx).difference(bin_idx))\n",
    "    col_names = np.array(header_names)\n",
    "    nominal_cols = col_names[nominal_idx].tolist()\n",
    "    binary_cols = col_names[bin_idx].tolist()\n",
    "    numeric_cols = col_names[numeric_idx].tolist()\n",
    "    return nominal_cols, binary_cols, numeric_cols\n",
    "\n",
    "nominal_cols, binary_cols, numeric_cols = getColunssByClass(header_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 - Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.2.1- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is already imported to a Panda DataFrame, our current task is cleaning it and prepare to be consumed by an ML system. First; we will look for inconsistent columns. For example, when we check the 'su_attempt' column, we can see 3 possible values: 0, 1,2. As each line is related to one transaction, in each transaction, you can have only two values: successful (1) and failure (0); it does not make sense the value 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_df.groupby(['su_attempted']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, now we find that the field 'num_outbound_cmds' has a unique value. It does not make sense to include this field in the ML system, as it is a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['num_outbound_cmds']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I will fix the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data (train_df, test_df, num_cols):\n",
    "    train_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "    test_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "    print(train_df.groupby(['su_attempted']).size())\n",
    "\n",
    "    train_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "    test_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "    num_cols.remove('num_outbound_cmds')\n",
    "\n",
    "cleaning_data(train_df,test_df,numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the attack distribution in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_dataset(dataset, label_name, title):\n",
    "    plt_value = train_df[label_name].value_counts()\n",
    "    plt_value.plot(kind='barh', figsize=(15, 10), fontsize=16, title=title)\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(train_df,'attack_type','Training Attack Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(train_df,'attack_category','Training Attack Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(test_df,'attack_type','Test Attack Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(test_df,'attack_category','Test Attack Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.2.2- Adjusting and Normalizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to separate labels from the other fields and drop them because they are not required for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to convert the categorical data to numeric ones. The reason is that the ML models process better numeric values.\n",
    "\n",
    "Ti support it, we can encode our categorical column using the **One-hot encoding**. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded columns.\n",
    "\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(train_df, test_df, target_name,new_target_name, nominal_cols):\n",
    "\n",
    "    #split the dataset into target_labels and values\n",
    "    train_label = train_df[new_target_name]\n",
    "    train_x_raw = train_df.drop([new_target_name, target_name], axis=1)\n",
    "\n",
    "    test_label = test_df[new_target_name]\n",
    "    test_x_raw = test_df.drop([new_target_name, target_name], axis=1)\n",
    "\n",
    "    #convert the categorical values to numeric values\n",
    "    combined_df_raw = pd.concat([train_x_raw, test_x_raw])\n",
    "    combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "    train_x = combined_df[:len(train_x_raw)]\n",
    "    test_x = combined_df[len(train_x_raw):]\n",
    "\n",
    "    return train_x,train_label, test_x, test_label\n",
    "\n",
    "train_x,train_label, test_x, test_label = prepare_dataset(train_df, test_df, 'attack_type','attack_category', nominal_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, we scaled the numeric values to rearrange them to be in the same range.\n",
    "\n",
    "The scaling process that we will use is **standardization** (or **z-score normalization**); it scales the values while considering standard deviation. If the standard deviation of features is different, their range also will differ from each other. This reduces the effect of the outliers in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scalling(train_x, test_x, num_cols):\n",
    "    # scaller  numeric values\n",
    "    standard_scaler = StandardScaler().fit(train_x[num_cols])\n",
    "    train_x[num_cols] = standard_scaler.transform(train_x[num_cols])\n",
    "    test_x[num_cols] = standard_scaler.transform(test_x[num_cols])\n",
    "    \n",
    "    return train_x,  test_x\n",
    "\n",
    "train_x,  test_x = scalling(train_x, test_x,numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is encoding the labels, enabling that, after the clustering phase, we can convert the values to the initial classes that our database has (dos, r2l, u2r, probe, and normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def enconding_label(train_label, test_label):\n",
    "    # encoder the labels and transform in numeric values\n",
    "    # it is impoirtant to analyze the result\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_label_np = train_label.to_numpy()\n",
    "    train_label_encod = label_encoder.fit_transform(train_label_np)\n",
    "\n",
    "    test_label_np = test_label.to_numpy()\n",
    "    test_label_encod = label_encoder.fit_transform(test_label_np)\n",
    "\n",
    "    return train_label_encod, test_label_encod, label_encoder\n",
    "\n",
    "train_label_encod, test_label_encod, label_encoder = enconding_label(train_label,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 - Run Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.3.1 - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last phases prepare the data to be consumed by the ML system. Now, our works are to training the selected cluster algorithm. \n",
    "\n",
    "We select the **K-means** as the clustering algorithm. K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.\n",
    "\n",
    "To implement this algorithm is required to select the K value, which means the number of clusters that the model will try to cluster the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def trainning (train_x, cluster_number):\n",
    "    kmeans = KMeans(n_clusters=cluster_number, random_state=17).fit(train_x)\n",
    "    return kmeans\n",
    "\n",
    "number_of_cluster = 5\n",
    "kmeans = trainning(train_x,number_of_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.3.2. Predict Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, the next step is to start to use the model to predict values. We will use the test_set to predict attacks using the clustering ML. Perceives that we have the predict labels and the predict encoded labels. We need the predict labels to be used in the ML; however, to plot, as we will see ahead, it is required the encoded values of label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(kmeans,values_to_predict,label_encoder):\n",
    "    pred_encoded_labels = kmeans.predict(values_to_predict)\n",
    "    pred_labels = label_encoder.inverse_transform(pred_encoded_labels)\n",
    "    return pred_labels, pred_encoded_labels\n",
    "\n",
    "pred_labels, pred_encoded_labels = predict(kmeans,test_x,label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 - Interpret Result and Adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the result of our predictions. To plot the graph, we must use an algorithm that enables us to reduce the dimensionality of our results - the **Principal Component Analysis (PCA)**.\n",
    "\n",
    "PCA is a dimensionality reduction method, feature extraction that transforms the data from *“d-dimensional space”* into a new coordinate system of dimension **p**, where **p <= d**.\n",
    "\n",
    "ML models with many input variables or higher dimensionality tend to fail when operating on a higher input dataset. \n",
    "PCA helps in identifying relationships among different variables & then coupling them.  PCA involves transforming variables in the dataset into a new set of variables called PCs (Principal Components). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_results(data_x, data_label, pred_enconded_labels, label_encoder, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(data_x)\n",
    "\n",
    "    ds = pd.DataFrame(\n",
    "        data=principalComponents,\n",
    "        columns=[\"component_1\", \"component_2\"],\n",
    "    )\n",
    "\n",
    "    ds[\"predicted_cluster\"] = pred_enconded_labels\n",
    "    ds[\"true_label\"] = label_encoder.inverse_transform(data_label)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    scat = sns.scatterplot(\n",
    "        data=ds,\n",
    "        x=\"component_1\",\n",
    "        y=\"component_2\",\n",
    "        s=50,\n",
    "        hue=\"predicted_cluster\",\n",
    "        style=\"true_label\",\n",
    "        palette=\"Set2\",\n",
    "\n",
    "    )\n",
    "    scat.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_results(test_x, test_label_encod, pred_encoded_labels, label_encoder,'Clustering results from NDD dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result presents a set of problems in our clusterization. However, we need to remember that our idea is to check how data is similar, considering that an enemy can compromise previous dataset classification.\n",
    "\n",
    "In the graph, you can see the clusters (colors) and the attack's classes (symbols). It is possible to check that different classes are clustered in the same group.\n",
    "\n",
    "It indicates a good chance that the model does not cluster the data well, making required a formal evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.4.1. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task in the model evaluation is to calculate four metrics to cluster: *Silhouette Coefficient score, Rand Index, Completeness, Homogeinity, and V-Measure*.\n",
    "\n",
    "The **Silhouette Coefficient** is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The best value is 1, and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "The **Rand Index** computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs assigned in the same or different clusters in the predicted and true clustering. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical.\n",
    "\n",
    "**Homogeneity**: It estimates how many of the clusters predicted contain only members of a single class. \n",
    "\n",
    "**Completeness**: measures whether all members of a given class are assigned to the same cluster.\n",
    "\n",
    "**Validity-Measure (V-Measure)**: is the harmonic mean of homogeneity and completeness.\n",
    "\n",
    "*Homogeneity, Completeness, and V-Measure* have positive values between 0.0 and 1.0, larger values being desirable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score, \\\n",
    "    homogeneity_score, v_measure_score\n",
    "\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "def calc_metrics(data_x,data_labels, predicted_encoded_labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    pcomp = pca.fit_transform(data_x)\n",
    "    print('Silhouette Coefficient score: {}'.format(silhouette_score(pcomp, predicted_encoded_labels)))\n",
    "    print('Rand Index: {}'.format(adjusted_rand_score(data_labels, predicted_encoded_labels)))\n",
    "    print('Completeness: {}'.format(completeness_score(data_labels, predicted_encoded_labels)))\n",
    "    print('Homogeneity: {}'.format(homogeneity_score(data_labels, predicted_encoded_labels)))\n",
    "    print('V-measure: {}'.format(v_measure_score(data_labels, predicted_encoded_labels)))\n",
    "    print('Total number of features: {}'.format(len(data_x.columns)))\n",
    "\n",
    "calc_metrics(test_x,test_label_encod,pred_encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the bad result of the metrics, we need to analyze if the model has a better result if I use a fewer cluster.\n",
    "\n",
    "For this reason, I will plot what happens with the metrics if I use a different K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_performance(data_x, data_label, label_encoder, range_init, range_final):\n",
    "    # Empty lists to hold evaluation metrics\n",
    "    silhouette_scores = []\n",
    "    ari_scores = []\n",
    "    homo_array = []\n",
    "    compl_array = []\n",
    "    v_meas_arrays = []\n",
    "\n",
    "    for n in range(range_init, len(label_encoder.classes_)):\n",
    "        # This set the number of components for pca,\n",
    "        # but leaves other steps unchanged\n",
    "        pca = PCA(n_components=2)\n",
    "        pcomp = pca.fit_transform(data_x)\n",
    "\n",
    "        kmeans = trainning(data_x,n)\n",
    "        pred_labels, pred_encoded_labels= predict(kmeans,data_x,label_encoder)\n",
    "        sil_coef=silhouette_score(pcomp, pred_encoded_labels)\n",
    "        ari = adjusted_rand_score(data_label, pred_encoded_labels)\n",
    "        compl = completeness_score(data_label, pred_encoded_labels)\n",
    "        homo= homogeneity_score(data_label, pred_encoded_labels)\n",
    "        v= v_measure_score(data_label, pred_encoded_labels)\n",
    "\n",
    "        # Add metrics to their lists\n",
    "        silhouette_scores.append(sil_coef)\n",
    "        ari_scores.append(ari)\n",
    "        compl_array.append(compl)\n",
    "        homo_array.append(homo)\n",
    "        v_meas_arrays.append(v)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(\n",
    "        range(range_init, len(label_encoder.classes_)),\n",
    "        silhouette_scores,\n",
    "        c=\"#008fd5\",\n",
    "        label=\"Silhouette Coefficient\",\n",
    "    )\n",
    "    plt.plot(range(range_init, len(label_encoder.classes_)), ari_scores, c=\"b\", label=\"ARI\")\n",
    "\n",
    "    plt.plot(range(range_init, len(label_encoder.classes_)), compl_array, c=\"g\", label=\"Completeness Score\")\n",
    "    plt.plot(range(range_init, len(label_encoder.classes_)), homo_array, c=\"r\", label=\"Homogeneity Score\")\n",
    "    plt.plot(range(range_init, len(label_encoder.classes_)), v_meas_arrays, c=\"gray\", label=\"V Measure Score\")\n",
    "\n",
    "    plt.xlabel(\"n_components\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Clustering Performance\\nas a Function of n_components\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_clustering_performance(test_x,test_label_encod,label_encoder,2,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph, we can conclude that the best cost-effective solution is to use a K=3. The next step is to re-train the model, rerun it, and check the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
